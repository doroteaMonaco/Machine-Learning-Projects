{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7TClOJXcJst"
   },
   "source": [
    "First, we can load the dataset and create the validation set as 10% of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20TI-ewhcRnO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXB3BPy7e8tV"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "original_train_set = datasets.MNIST(\n",
    "    root='./drive/MyDrive', train=True, download=True, transform=transform_train\n",
    ")\n",
    "\n",
    "test_set = datasets.MNIST(\n",
    "    root='./drive/MyDrive', train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "train_indexes, val_indexes = train_test_split(\n",
    "    range(len(original_train_set)), test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "train_set = Subset(original_train_set, train_indexes)\n",
    "validation_set = Subset(original_train_set, val_indexes)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKnYiGTIgSMc"
   },
   "source": [
    "Now, we are defining the MPL network which consists in 2 hidden layers and 1 output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4BpX4ukgnaV"
   },
   "outputs": [],
   "source": [
    "n1 = 256\n",
    "n2 = 128\n",
    "N_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQD0XrEJrJ2w"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(28*28, n1)\n",
    "    self.fc2 = nn.Linear(n1, n2)\n",
    "    self.fc3 = nn.Linear(n2, N_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4LVNbz0sAjl"
   },
   "source": [
    "Now we will train this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7OrAEvOsGKF"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "EPOCHS = 30\n",
    "lr1 = 1\n",
    "lr2 = 0.1\n",
    "lr3 = 0.01\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xe6wyJE-iEn3"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, validation_loader, criterion, optimizer):\n",
    "  train_losses = []\n",
    "  validation_losses = []\n",
    "\n",
    "  train_accuracy = []\n",
    "  validation_accuracy = []\n",
    "\n",
    "  model.to(device)\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    corrects = 0\n",
    "    losses = 0\n",
    "    samples = 0\n",
    "    print(f'Epoch {epoch}')\n",
    "    for i, batch in enumerate(train_loader):\n",
    "      samples += batch[0].shape[0] #the first part of the tensor contains images of the batch\n",
    "      images, labels = batch\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      predictions = model(images) #as output I obtain the logits\n",
    "\n",
    "      _, label_pred = torch.max(predictions, 1) #index of the predicted class\n",
    "      corrects += torch.sum(labels == label_pred).item()\n",
    "\n",
    "      loss = criterion(predictions, labels) #mean of the loss in the batch\n",
    "      losses += loss.item() * batch[0].shape[0] #weigthed mean\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    train_losses.append(losses / samples)\n",
    "    print(f'Train loss {epoch}: {losses/samples:.4f}')\n",
    "    train_accuracy.append(1.0*corrects / float(samples))\n",
    "\n",
    "    model.eval()\n",
    "    corrects_val = 0\n",
    "    losses_val = 0\n",
    "    samples_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for batch in validation_loader:\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        samples_val += batch[0].shape[0]\n",
    "        predictions = model(images)\n",
    "        _, label_pred = torch.max(predictions, 1)\n",
    "        corrects_val += torch.sum(labels == label_pred).item()\n",
    "        loss = criterion(predictions, labels)\n",
    "        losses_val += loss.item() * batch[0].shape[0]\n",
    "      validation_losses.append(losses_val / samples_val)\n",
    "      print(f'Validation loss {epoch}: {losses_val/samples_val:.4f}')\n",
    "      validation_accuracy.append(1.0*corrects_val / float(samples_val))\n",
    "\n",
    "  return train_losses, validation_losses, train_accuracy, validation_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-0wFGINn23E"
   },
   "source": [
    "Now we'll perform the train with different values of learning rates and same number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaM2ePd_n_Ty"
   },
   "source": [
    "1) Learning Rate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "1shvEoheoCnr",
    "outputId": "d5124ad8-38fb-4446-a313-f934c49698f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    }
   ],
   "source": [
    "net = MLP()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr1, momentum=0.9)\n",
    "\n",
    "validation_losses1, train_losses1, train_accuracy1, validation_accuracy1 = train(net, train_loader, validation_loader, criterion, optimizer)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].plot(train_losses1, c='blue', label='train')\n",
    "ax[0].plot(validation_losses1, c='red', label='validation')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(train_accuracy1, c='blue', label='train')\n",
    "ax[1].plot(validation_accuracy1, c='red', label='validation')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gMbF9dEsBo0"
   },
   "source": [
    "The learning rate is too high so we have gradient explosion.\n",
    "Now we will train the model with learning rate 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rPmI1-dsM4I"
   },
   "outputs": [],
   "source": [
    "net = MLP()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr2, momentum=0.9)\n",
    "\n",
    "validation_losses2, train_losses2, train_accuracy2, validation_accuracy2 = train(net, train_loader, validation_loader, criterion, optimizer)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].plot(train_losses2, c='blue', label='train')\n",
    "ax[0].plot(validation_losses2, c='red', label='validation')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(train_accuracy2, c='blue', label='train')\n",
    "ax[1].plot(validation_accuracy2, c='red', label='validation')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n00gs7rXuA2B"
   },
   "source": [
    "It is better then the first model.\n",
    "Now we will train with learning rate 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXG9FVkDuPvo"
   },
   "outputs": [],
   "source": [
    "net = MLP()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr3, momentum=0.9)\n",
    "\n",
    "validation_losses3, train_losses3, train_accuracy3, validation_accuracy3 = train(net, train_loader, validation_loader, criterion, optimizer)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].plot(train_losses3, c='blue', label='train')\n",
    "ax[0].plot(validation_losses3, c='red', label='validation')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(train_accuracy3, c='blue', label='train')\n",
    "ax[1].plot(validation_accuracy3, c='red', label='validation')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaOJhFb6wEBP"
   },
   "source": [
    "It is worst tha the second model, because the train curve and the validation curve diverge, this means that there is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcypT7NGwcJf"
   },
   "source": [
    "**Now, let's build a CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvkzG0nHwp3F"
   },
   "outputs": [],
   "source": [
    "n1 = 64\n",
    "n2 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8mcEBmWwucc"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self, n1, n2):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(1, n1, 3, padding=1)\n",
    "    self.norm1 = nn.BatchNorm2d(n1)\n",
    "    self.conv2 = nn.Conv2d(n1, n2, 3, padding=1)\n",
    "    self.norm2 = nn.BatchNorm2d(n2)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.fc = nn.Linear(n2*7*7, 10)\n",
    "    self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.norm1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.pool(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.norm2(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.pool(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uT1_Q9ozDlF"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 1 required positional argument: 'version'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m net \u001b[38;5;241m=\u001b[39m CNN(n1, n2)\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr2, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m validation_losses, train_losses, train_accuracy, validation_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      7\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(train_losses, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: train() missing 1 required positional argument: 'version'"
     ]
    }
   ],
   "source": [
    "net = CNN(n1, n2)\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr2, momentum=0.9)\n",
    "\n",
    "validation_losses, train_losses, train_accuracy, validation_accuracy = train(net, train_loader, validation_loader, criterion, optimizer)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].plot(train_losses, c='blue', label='train')\n",
    "ax[0].plot(validation_losses, c='red', label='validation')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(train_accuracy, c='blue', label='train')\n",
    "ax[1].plot(validation_accuracy, c='red', label='validation')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWR74uAItBnd"
   },
   "source": [
    "Firstly, I tried the training of the network with a simple CNN, composed by two convutional layers, a pool layer and a fully connected layer. But there was overfitting, so I added two Batch Norm layer after the convutional ones and a dropout layer after the full connected one. The CNN work much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7vjcD-JXSlQ"
   },
   "source": [
    "**Hyperparamter Optimization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fS0Vi5KHX7bI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train() missing 1 required positional argument: 'version'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m   optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mRMSprop(params_model, lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     49\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 50\u001b[0m _, _, accuracy, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: params})\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: train() missing 1 required positional argument: 'version'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "parameters = {\n",
    "    'n1': [32, 64, 128, 256],\n",
    "    'n2': [32, 64, 128, 256],\n",
    "    'lr': [0.1, 0.01, 0.001, 0.0001],\n",
    "    'weight_decay': [0, 0.0001, 0.001, 0.01],\n",
    "    'batch_size': [64, 128, 256, 512],\n",
    "    'optimizer': ['SGD', 'Adam', 'RMSprop'],\n",
    "    'momentum': [0.95, 0.9, 0.8]\n",
    "}\n",
    "\n",
    "Niteration = 10\n",
    "bestAccuracy = 0\n",
    "bestParameters = {}\n",
    "results = []\n",
    "\n",
    "for i in range(Niteration):\n",
    "  print(f'Iteration {i}')\n",
    "  params = {\n",
    "      'n1': random.choice(parameters['n1']),\n",
    "      'n2': random.choice(parameters['n2']),\n",
    "      'lr': random.choice(parameters['lr']),\n",
    "      'weight_decay': random.choice(parameters['weight_decay']),\n",
    "      'batch_size': random.choice(parameters['batch_size']),\n",
    "      'optimizer': random.choice(parameters['optimizer']),\n",
    "      'momentum': random.choice(parameters['momentum'])\n",
    "  }\n",
    "\n",
    "  train_loader = DataLoader(train_set, batch_size=params['batch_size'], shuffle=True, num_workers=2, drop_last=True)\n",
    "  validation_loader = DataLoader(validation_set, batch_size=params['batch_size'], shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "  net = CNN(params['n1'], params['n2'])\n",
    "\n",
    "  params_model = net.parameters()\n",
    "  lr = params['lr']\n",
    "\n",
    "  if params['optimizer'] == 'SGD':\n",
    "    optimizer = optim.SGD(params_model, lr=lr, momentum=params['momentum'], weight_decay=params['weight_decay'])\n",
    "  elif params['optimizer'] == 'Adam':\n",
    "    optimizer = optim.Adam(params_model, lr=lr, weight_decay=params['weight_decay'])\n",
    "  elif params['optimizer'] == 'RMSprop':\n",
    "    optimizer = optim.RMSprop(params_model, lr=lr, weight_decay=params['weight_decay'])\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  _, _, accuracy, _ = train(net, train_loader, validation_loader, criterion, optimizer)\n",
    "\n",
    "  results.append({'accuracy': accuracy, 'params': params})\n",
    "  print(f'Accuracy: {accuracy[-1]}')\n",
    "\n",
    "  if accuracy[-1] > bestAccuracy:\n",
    "    bestAccuracy = accuracy[-1]\n",
    "    bestParameters = params\n",
    "\n",
    "print('Best accuracy:', bestAccuracy)\n",
    "for k, v in bestParameters.items():\n",
    "  print(k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwaRqDSYlEBh"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRvDIkUhjqP4"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "n1 = 32\n",
    "n2 = 128\n",
    "lr = 0.001\n",
    "weight_decay = 0\n",
    "batch_size = 64\n",
    "momentum = 0.9\n",
    "\n",
    "net1 = CNN(n1, n2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(net1.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "trainLosses, valLosses, trainAccuracy, validationAccuracy = train(net1, train_loader, validation_loader, criterion, optimizer, 1)\n",
    "\n",
    "print(f'Accuracy on validation set: {validationAccuracy[-1]}')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].plot(trainLosses, c='blue', label='train_loss')\n",
    "ax[0].plot(valLosses, c='red', label='val_loss')\n",
    "ax[1].plot(trainAccuracy, c='blue', label='train_accuracy')\n",
    "ax[1].plot(validationAccuracy, c='red', label='val_accuracy')\n",
    "\n",
    "ax[0].set_xlabel('num epochs')\n",
    "ax[0].set_ylabel('loss')\n",
    "\n",
    "ax[1].set_xlabel('num epochs')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amKJfu0qmEJ8"
   },
   "source": [
    "**First Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "tI-HWaBrmHv4",
    "outputId": "2233530e-e4a2-4862-91cc-725e95c5cd63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "n1 = 32\n",
    "n2 = 128\n",
    "lr = 0.001\n",
    "weight_decay = 0\n",
    "batch_size = 64\n",
    "momentum = 0.9\n",
    "\n",
    "net1 = CNN(n1, n2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(net1.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "trainLosses, valLosses, trainAccuracy, validationAccuracy = train(net1, train_loader, validation_loader, criterion, optimizer, 1)\n",
    "\n",
    "print(f'Accuracy on validation set: {validationAccuracy[-1]}')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].plot(trainLosses, c='blue', label='train_loss')\n",
    "ax[0].plot(valLosses, c='red', label='val_loss')\n",
    "ax[1].plot(trainAccuracy, c='blue', label='train_accuracy')\n",
    "ax[1].plot(validationAccuracy, c='red', label='val_accuracy')\n",
    "\n",
    "ax[0].set_xlabel('num epochs')\n",
    "ax[0].set_ylabel('loss')\n",
    "\n",
    "ax[1].set_xlabel('num epochs')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLaJcsTGzDKg"
   },
   "source": [
    "**Second Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEKZTuWNzFK8"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "n1 = 32\n",
    "n2 = 128\n",
    "lr = 0.001\n",
    "weight_decay = 0\n",
    "batch_size = 64\n",
    "momentum = 0.9\n",
    "\n",
    "net2 = CNN(n1, n2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(net2.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "trainLosses, valLosses, trainAccuracy, validationAccuracy = train(net2, train_loader, validation_loader, criterion, optimizer, 2)\n",
    "\n",
    "print(f'Accuracy on validation set: {validationAccuracy[-1]}')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].plot(trainLosses, c='blue', label='train_loss')\n",
    "ax[0].plot(valLosses, c='red', label='val_loss')\n",
    "ax[1].plot(trainAccuracy, c='blue', label='train_accuracy')\n",
    "ax[1].plot(validationAccuracy, c='red', label='val_accuracy')\n",
    "\n",
    "ax[0].set_xlabel('num epochs')\n",
    "ax[0].set_ylabel('loss')\n",
    "\n",
    "ax[1].set_xlabel('num epochs')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQsyZSDjzGab"
   },
   "source": [
    "**Third Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABX2B2I-zIMk"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "n1 = 32\n",
    "n2 = 128\n",
    "lr = 0.001\n",
    "weight_decay = 0\n",
    "batch_size = 64\n",
    "momentum = 0.9\n",
    "\n",
    "net3 = CNN(n1, n2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(net3.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "trainLosses, valLosses, trainAccuracy, validationAccuracy = train(net3, train_loader, validation_loader, criterion, optimizer, 3)\n",
    "\n",
    "print(f'Accuracy on validation set: {validationAccuracy[-1]}')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].plot(trainLosses, c='blue', label='train_loss')\n",
    "ax[0].plot(valLosses, c='red', label='val_loss')\n",
    "ax[1].plot(trainAccuracy, c='blue', label='train_accuracy')\n",
    "ax[1].plot(validationAccuracy, c='red', label='val_accuracy')\n",
    "\n",
    "ax[0].set_xlabel('num epochs')\n",
    "ax[0].set_ylabel('loss')\n",
    "\n",
    "ax[1].set_xlabel('num epochs')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BKQbNQn6L2k"
   },
   "source": [
    "Finally, we can test the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpmB9omx6Qqi"
   },
   "source": [
    "**TEST MODEL 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "4cuQtzf96T6N",
    "outputId": "585876a4-e0af-4dd8-d83b-a832e837515e"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model1.pht'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-510685316.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcorrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnet1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model1.pht'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnet1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model1.pht'"
     ]
    }
   ],
   "source": [
    "losses = 0\n",
    "samples = 0\n",
    "corrects = 0\n",
    "\n",
    "net1.load_state_dict(torch.load('./drive/MyDrive/model1.pth'))\n",
    "\n",
    "net1.to(device)\n",
    "net1.eval()\n",
    "with torch.no_grad():\n",
    "  for batch in test_loader:\n",
    "    images, labels = batch\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    samples += batch[0].shape[0]\n",
    "    preds = net1(images)\n",
    "\n",
    "    _, label_pred = torch.max(preds.data, 1)\n",
    "    corrects += torch.sum(labels == label_pred).item()\n",
    "acc = 100 * 1.0 * corrects / float(samples)\n",
    "print(f'Accuracy on test set 1: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZtjsrT47FyU"
   },
   "source": [
    "**TEST MODEL 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUYMmvlT7HrZ"
   },
   "outputs": [],
   "source": [
    "losses = 0\n",
    "samples = 0\n",
    "corrects = 0\n",
    "\n",
    "net2.load_state_dict(torch.load('./drive/MyDrive/model2.pth'))\n",
    "\n",
    "net2.to(device)\n",
    "net2.eval()\n",
    "with torch.no_grad():\n",
    "  for batch in test_loader:\n",
    "    images, labels = batch\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    samples += batch[0].shape[0]\n",
    "    preds = net2(images)  \n",
    "\n",
    "    _, label_pred = torch.max(preds.data, 1)\n",
    "    corrects += torch.sum(labels == label_pred).item()\n",
    "acc = 100 * 1.0 * corrects / float(samples)\n",
    "print(f'Accuracy on test set 2: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZAPeROM7VpN"
   },
   "source": [
    "**TEST MODEL 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgWytFYm7XWm"
   },
   "outputs": [],
   "source": [
    "losses = 0\n",
    "samples = 0\n",
    "corrects = 0\n",
    "\n",
    "net3.load_state_dict(torch.load('./drive/MyDrive/model3.pth'))\n",
    "\n",
    "net3.to(device)\n",
    "net3.eval()\n",
    "with torch.no_grad():\n",
    "  for batch in test_loader:\n",
    "    images, labels = batch\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    samples += batch[0].shape[0]\n",
    "    preds = net3(images)  \n",
    "\n",
    "    _, label_pred = torch.max(preds.data, 1)\n",
    "    corrects += torch.sum(labels == label_pred).item()\n",
    "acc = 100 * 1.0 * corrects / float(samples)\n",
    "print(f'Accuracy on test set 3: {acc}')  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
